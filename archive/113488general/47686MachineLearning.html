---
layout: page
title: Lean Prover Zulip Chat Archive 
permalink: archive/113488general/47686MachineLearning.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/113488general/index.html">general</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html">Machine Learning</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">
{% raw %}
<a name="124463630"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463630" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463630">Simon Hudon (Mar 31 2018 at 21:21)</a>:</h4>
<p><span class="user-mention" data-user-id="110025">@Andrew Ashworth</span> Let's switch threads</p>

<a name="124463678"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463678" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463678">Simon Hudon (Mar 31 2018 at 21:23)</a>:</h4>
<p>You should consider changing names <span class="emoji emoji-1f61d" title="stuck out tongue closed eyes">:stuck_out_tongue_closed_eyes:</span></p>

<a name="124463722"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463722" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463722">Andrew Ashworth (Mar 31 2018 at 21:24)</a>:</h4>
<p>but yeah, ml isn't going to break into real time radar imaging anytime soon, mostly because you need a ton of compute horsepower</p>

<a name="124463729"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463729" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463729">Simon Hudon (Mar 31 2018 at 21:25)</a>:</h4>
<p>if it did, what would it improve?</p>

<a name="124463730"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463730" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463730">Andrew Ashworth (Mar 31 2018 at 21:25)</a>:</h4>
<p>depends on what your end application is</p>

<a name="124463771"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463771" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463771">Andrew Ashworth (Mar 31 2018 at 21:26)</a>:</h4>
<p>for radar, a big problem is distinguishing objects from noise</p>

<a name="124463774"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463774" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463774">Andrew Ashworth (Mar 31 2018 at 21:26)</a>:</h4>
<p>we use tons of linear approximations everywhere that aren't totally applicable</p>

<a name="124463782"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463782" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463782">Andrew Ashworth (Mar 31 2018 at 21:27)</a>:</h4>
<p>but who cares because everything is a gaussian if you actually want to compute things</p>

<a name="124463788"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463788" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463788">Andrew Ashworth (Mar 31 2018 at 21:27)</a>:</h4>
<p>unfortunately noise is rarely linear, rarely uncorrelated, and better ways of doing it are too complicated to do in real time</p>

<a name="124463828"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463828" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463828">Andrew Ashworth (Mar 31 2018 at 21:28)</a>:</h4>
<p>but maybe a well trained ml model would be efficient enough? I don't know, but i think it might be true in the future</p>

<a name="124463831"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463831" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463831">Andrew Ashworth (Mar 31 2018 at 21:28)</a>:</h4>
<p>gotta stay ahead of the curve and do some learning :)</p>

<a name="124463833"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463833" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463833">Simon Hudon (Mar 31 2018 at 21:28)</a>:</h4>
<p>If it was computationally feasible, would it bring in safety risks?</p>

<a name="124463841"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463841" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463841">Andrew Ashworth (Mar 31 2018 at 21:29)</a>:</h4>
<p>hmm, possibly, but we already accept some risk of error in the process</p>

<a name="124463843"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463843" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463843">Andrew Ashworth (Mar 31 2018 at 21:29)</a>:</h4>
<p>since it's statistical, we design the system to some probability pf of a false alarm</p>

<a name="124463845"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463845" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463845">Andrew Ashworth (Mar 31 2018 at 21:29)</a>:</h4>
<p>(of detecting an object, in this example)</p>

<a name="124463887"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463887" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463887">Andrew Ashworth (Mar 31 2018 at 21:30)</a>:</h4>
<p>can you talk about that false alarm probability in a deep learning model? kinda... sorta... I don't know well enough to say that rigorously</p>

<a name="124463888"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463888" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463888">Simon Hudon (Mar 31 2018 at 21:30)</a>:</h4>
<p>Already having an estimate for the probability of failure of a ML model seems pretty difficult, isn't it?</p>

<a name="124463938"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463938" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463938">Andrew Ashworth (Mar 31 2018 at 21:32)</a>:</h4>
<p>yup. but even a handwavy one would be nice though</p>

<a name="124463947"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463947" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463947">Andrew Ashworth (Mar 31 2018 at 21:33)</a>:</h4>
<p>i mean, current estimates are based on assuming gaussian noise, which doesn't reflect reality</p>

<a name="124463948"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463948" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463948">Andrew Ashworth (Mar 31 2018 at 21:33)</a>:</h4>
<p>it's a complicated subject</p>

<a name="124463949"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463949" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463949">Simon Hudon (Mar 31 2018 at 21:33)</a>:</h4>
<p>I'm wondering if the best that can be done with an ML system is counting errors and retiring / replacing models whose number of errors exceeds the tolerance. I'm not sure if it's any good though</p>

<a name="124463994"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463994" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463994">Simon Hudon (Mar 31 2018 at 21:34)</a>:</h4>
<p>Sorry, I might not be focusing on what you care about, I think a lot about guarantees. In your case, the hope seems to be an increase in average accuracy, does that make sense?</p>

<a name="124464000"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464000" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464000">Andrew Ashworth (Mar 31 2018 at 21:35)</a>:</h4>
<p>yeah. it doesn't make sense to speak of guarantees in object detection, just minimizing the ratio of detections to false alarms</p>

<a name="124464040"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464040" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464040">Simon Hudon (Mar 31 2018 at 21:36)</a>:</h4>
<p>You could think of a guarantee as an upper bound on the probability of failure</p>

<a name="124464089"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464089" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464089">Simon Hudon (Mar 31 2018 at 21:38)</a>:</h4>
<p>Whenever I think of ML lately, I think of a software tool that was on the news recently where judges would use ML to help them make their judgement faster. It was found that the process exaggerated an existing racial bias. I see that as a conflict between efficiency and correctness. If you can't make any statement about the correctness of the software, what good is it?</p>

<a name="124464095"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464095" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464095">Andrew Ashworth (Mar 31 2018 at 21:39)</a>:</h4>
<p>haha, that's a dangerous statement to make</p>

<a name="124464097"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464097" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464097">Andrew Ashworth (Mar 31 2018 at 21:39)</a>:</h4>
<p>cowboy c coders brought us the computer revolution :)</p>

<a name="124464099"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464099" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464099">Simon Hudon (Mar 31 2018 at 21:39)</a>:</h4>
<p>My skin has grown impervious to the rocks I'm being thrown <span class="emoji emoji-1f61c" title="stuck out tongue winking eye">:stuck_out_tongue_winking_eye:</span></p>

<a name="124464144"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464144" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464144">Simon Hudon (Mar 31 2018 at 21:40)</a>:</h4>
<p>Which revolution do you mean? The age of personal computing or putting white supremacists in the White House?</p>

<a name="124464192"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464192" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464192">Andrew Ashworth (Mar 31 2018 at 21:43)</a>:</h4>
<p>both, because as you know, it's a fact that automation has put many out of work, but that's getting a little off-topic</p>

<a name="124464241"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464241" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464241">Simon Hudon (Mar 31 2018 at 21:44)</a>:</h4>
<p>That's true (both things)</p>

<a name="124464249"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464249" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464249">Andrew Ashworth (Mar 31 2018 at 21:45)</a>:</h4>
<blockquote>
<p>I'm wondering if the best that can be done with an ML system is counting errors and retiring / replacing models whose number of errors exceeds the tolerance. I'm not sure if it's any good though</p>
</blockquote>
<p>back in the day, when humans interpreted radar returns, firing a bad operator was exactly what was done</p>

<a name="124464292"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464292" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464292">Simon Hudon (Mar 31 2018 at 21:46)</a>:</h4>
<p>I'm slowly getting to respect the development of cowboy developers and I realize that everything doesn't need to be bullet proof. But there is an extent where rolling out a technology should be after proper study</p>


{% endraw %}
