---
layout: archive
title: Lean Prover Zulip Chat Archive 
permalink: archive/113488general/47686MachineLearning.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/113488general/index.html">general</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html">Machine Learning</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">
{% raw %}
<a name="124463630"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463630" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463630">Simon Hudon (Mar 31 2018 at 19:21)</a>:</h4>
<p><span class="user-mention" data-user-id="110025">@Andrew Ashworth</span> Let's switch threads</p>

<a name="124463678"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463678" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463678">Simon Hudon (Mar 31 2018 at 19:23)</a>:</h4>
<p>You should consider changing names <span class="emoji emoji-1f61d" title="stuck out tongue closed eyes">:stuck_out_tongue_closed_eyes:</span></p>

<a name="124463722"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463722" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463722">Andrew Ashworth (Mar 31 2018 at 19:24)</a>:</h4>
<p>but yeah, ml isn't going to break into real time radar imaging anytime soon, mostly because you need a ton of compute horsepower</p>

<a name="124463729"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463729" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463729">Simon Hudon (Mar 31 2018 at 19:25)</a>:</h4>
<p>if it did, what would it improve?</p>

<a name="124463730"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463730" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463730">Andrew Ashworth (Mar 31 2018 at 19:25)</a>:</h4>
<p>depends on what your end application is</p>

<a name="124463771"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463771" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463771">Andrew Ashworth (Mar 31 2018 at 19:26)</a>:</h4>
<p>for radar, a big problem is distinguishing objects from noise</p>

<a name="124463774"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463774" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463774">Andrew Ashworth (Mar 31 2018 at 19:26)</a>:</h4>
<p>we use tons of linear approximations everywhere that aren't totally applicable</p>

<a name="124463782"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463782" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463782">Andrew Ashworth (Mar 31 2018 at 19:27)</a>:</h4>
<p>but who cares because everything is a gaussian if you actually want to compute things</p>

<a name="124463788"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463788" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463788">Andrew Ashworth (Mar 31 2018 at 19:27)</a>:</h4>
<p>unfortunately noise is rarely linear, rarely uncorrelated, and better ways of doing it are too complicated to do in real time</p>

<a name="124463828"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463828" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463828">Andrew Ashworth (Mar 31 2018 at 19:28)</a>:</h4>
<p>but maybe a well trained ml model would be efficient enough? I don't know, but i think it might be true in the future</p>

<a name="124463831"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463831" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463831">Andrew Ashworth (Mar 31 2018 at 19:28)</a>:</h4>
<p>gotta stay ahead of the curve and do some learning :)</p>

<a name="124463833"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463833" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463833">Simon Hudon (Mar 31 2018 at 19:28)</a>:</h4>
<p>If it was computationally feasible, would it bring in safety risks?</p>

<a name="124463841"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463841" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463841">Andrew Ashworth (Mar 31 2018 at 19:29)</a>:</h4>
<p>hmm, possibly, but we already accept some risk of error in the process</p>

<a name="124463843"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463843" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463843">Andrew Ashworth (Mar 31 2018 at 19:29)</a>:</h4>
<p>since it's statistical, we design the system to some probability pf of a false alarm</p>

<a name="124463845"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463845" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463845">Andrew Ashworth (Mar 31 2018 at 19:29)</a>:</h4>
<p>(of detecting an object, in this example)</p>

<a name="124463887"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463887" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463887">Andrew Ashworth (Mar 31 2018 at 19:30)</a>:</h4>
<p>can you talk about that false alarm probability in a deep learning model? kinda... sorta... I don't know well enough to say that rigorously</p>

<a name="124463888"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463888" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463888">Simon Hudon (Mar 31 2018 at 19:30)</a>:</h4>
<p>Already having an estimate for the probability of failure of a ML model seems pretty difficult, isn't it?</p>

<a name="124463938"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463938" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463938">Andrew Ashworth (Mar 31 2018 at 19:32)</a>:</h4>
<p>yup. but even a handwavy one would be nice though</p>

<a name="124463947"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463947" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463947">Andrew Ashworth (Mar 31 2018 at 19:33)</a>:</h4>
<p>i mean, current estimates are based on assuming gaussian noise, which doesn't reflect reality</p>

<a name="124463948"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463948" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463948">Andrew Ashworth (Mar 31 2018 at 19:33)</a>:</h4>
<p>it's a complicated subject</p>

<a name="124463949"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463949" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463949">Simon Hudon (Mar 31 2018 at 19:33)</a>:</h4>
<p>I'm wondering if the best that can be done with an ML system is counting errors and retiring / replacing models whose number of errors exceeds the tolerance. I'm not sure if it's any good though</p>

<a name="124463994"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124463994" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124463994">Simon Hudon (Mar 31 2018 at 19:34)</a>:</h4>
<p>Sorry, I might not be focusing on what you care about, I think a lot about guarantees. In your case, the hope seems to be an increase in average accuracy, does that make sense?</p>

<a name="124464000"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464000" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464000">Andrew Ashworth (Mar 31 2018 at 19:35)</a>:</h4>
<p>yeah. it doesn't make sense to speak of guarantees in object detection, just minimizing the ratio of detections to false alarms</p>

<a name="124464040"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464040" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464040">Simon Hudon (Mar 31 2018 at 19:36)</a>:</h4>
<p>You could think of a guarantee as an upper bound on the probability of failure</p>

<a name="124464089"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464089" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464089">Simon Hudon (Mar 31 2018 at 19:38)</a>:</h4>
<p>Whenever I think of ML lately, I think of a software tool that was on the news recently where judges would use ML to help them make their judgement faster. It was found that the process exaggerated an existing racial bias. I see that as a conflict between efficiency and correctness. If you can't make any statement about the correctness of the software, what good is it?</p>

<a name="124464095"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464095" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464095">Andrew Ashworth (Mar 31 2018 at 19:39)</a>:</h4>
<p>haha, that's a dangerous statement to make</p>

<a name="124464097"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464097" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464097">Andrew Ashworth (Mar 31 2018 at 19:39)</a>:</h4>
<p>cowboy c coders brought us the computer revolution :)</p>

<a name="124464099"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464099" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464099">Simon Hudon (Mar 31 2018 at 19:39)</a>:</h4>
<p>My skin has grown impervious to the rocks I'm being thrown <span class="emoji emoji-1f61c" title="stuck out tongue winking eye">:stuck_out_tongue_winking_eye:</span></p>

<a name="124464144"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464144" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464144">Simon Hudon (Mar 31 2018 at 19:40)</a>:</h4>
<p>Which revolution do you mean? The age of personal computing or putting white supremacists in the White House?</p>

<a name="124464192"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464192" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464192">Andrew Ashworth (Mar 31 2018 at 19:43)</a>:</h4>
<p>both, because as you know, it's a fact that automation has put many out of work, but that's getting a little off-topic</p>

<a name="124464241"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464241" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464241">Simon Hudon (Mar 31 2018 at 19:44)</a>:</h4>
<p>That's true (both things)</p>

<a name="124464249"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464249" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464249">Andrew Ashworth (Mar 31 2018 at 19:45)</a>:</h4>
<blockquote>
<p>I'm wondering if the best that can be done with an ML system is counting errors and retiring / replacing models whose number of errors exceeds the tolerance. I'm not sure if it's any good though</p>
</blockquote>
<p>back in the day, when humans interpreted radar returns, firing a bad operator was exactly what was done</p>

<a name="124464292"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Machine%20Learning/near/124464292" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/47686MachineLearning.html#124464292">Simon Hudon (Mar 31 2018 at 19:46)</a>:</h4>
<p>I'm slowly getting to respect the development of cowboy developers and I realize that everything doesn't need to be bullet proof. But there is an extent where rolling out a technology should be after proper study</p>


{% endraw %}
