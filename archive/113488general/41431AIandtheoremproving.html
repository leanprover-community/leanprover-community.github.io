---
layout: archive
title: Lean Prover Zulip Chat Archive 
permalink: archive/113488general/41431AIandtheoremproving.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/113488general/index.html">general</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html">AI and theorem proving</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">
{% raw %}
<a name="166042079"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042079" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042079">Jason Rute (May 19 2019 at 21:53)</a>:</h4>
<p>First, since I’ve only posted a few times in here, I thought I’d introduce myself.  I’m a former mathematician turned data scientist.  I used to work on computability theory and algorithmic randomness.  Jeremy Avigad was my advisor, which makes me an academic sibling to a couple of you.  Before graduate school, I worked on the Flyspeck project for a bit, but then lost interest in interactive theorem proving.  I’ve become more interested in it again with Lean and with the many advancements in AI and theorem proving.  </p>
<p>I’m especially interested in AI for theorem proving and I closely follow the field (as an outsider). I’m always happy to chat about it.  Since there have been many discussions here on the topic and I’d thought I’d share some resources and thoughts I have.  (Disclaimer: I am only an interested outsider.  I am not involved in any way with this research.)</p>

<a name="166042082"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042082" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042082">Jason Rute (May 19 2019 at 21:53)</a>:</h4>
<p><a href="https://slideslive.com/38909911/no-one-shall-drive-us-from-the-semantic-ai-paradise-of-computerunderstandable-math-and-science" target="_blank" title="https://slideslive.com/38909911/no-one-shall-drive-us-from-the-semantic-ai-paradise-of-computerunderstandable-math-and-science">This talk</a> by Joseph Urban is a great introduction to the state of field.  For more in depth, here are two <a href="http://phdopen.mimuw.edu.pl/index.php?page=z18w2" target="_blank" title="http://phdopen.mimuw.edu.pl/index.php?page=z18w2">sets of</a> <a href="http://cl-informatik.uibk.ac.at/teaching/ss18/mltp/content.php" target="_blank" title="http://cl-informatik.uibk.ac.at/teaching/ss18/mltp/content.php">lectures</a> on the subject by Cezary Kaliszyk.  (The first is just three lectures and has videos.)  Indeed, these two people and their respective labs seem to be leading the charge in AI for theorem proving.</p>
<p>Recently Berkeley, OpenAI, Google AI, and DeepMind are also getting into the field, which is really exciting.  Berkeley and OpenAI released <a href="https://github.com/ml4tp/gamepad" target="_blank" title="https://github.com/ml4tp/gamepad">GamePad</a> which is a learning environment to train AI agents in Coq.  Google AI just released a similar learning environment <a href="https://sites.google.com/view/holist/home" target="_blank" title="https://sites.google.com/view/holist/home">HOList</a> for HOL-Light (see slides at <a href="http://aitp-conference.org/2019/" target="_blank" title="http://aitp-conference.org/2019/">AITP 2019</a>).  Both of these projects provide datasets (e.g. the Flyspeck library) and some weak baselines.  As for DeepMind, Demis Hassabis recently mentioned “theorem proving” as one of the science projects they are working on right now (search “theorem” in <a href="https://cbmm.mit.edu/video/power-self-learning-systems" target="_blank" title="https://cbmm.mit.edu/video/power-self-learning-systems">this video/transcript</a>).</p>
<p>As for tactic-based ITPs, besides GamePad and HOList, there is <a href="https://github.com/HOL-Theorem-Prover/HOL/tree/master/src/tactictoe" target="_blank" title="https://github.com/HOL-Theorem-Prover/HOL/tree/master/src/tactictoe">TacticToe</a>, which is probably the most useful of the tools so far.  It fills in tactic proofs for HOL4 (<a href="https://arxiv.org/abs/1804.00596" target="_blank" title="https://arxiv.org/abs/1804.00596">paper</a>, slides/video at <a href="http://aitp-conference.org/2018/" target="_blank" title="http://aitp-conference.org/2018/">AITP 2018</a>).  Also, there appears to be <a href="https://github.com/data61/PSL" target="_blank" title="https://github.com/data61/PSL">PSL</a> for Isabelle, but I don't know much about it.</p>
<p>In another channel someone asked about using AlphaZero (or AlphaGo Zero) methods for theorem proving.  Despite what most people think, the AlphaZero algorithm isn’t so much about playing games as training an agent to search through a tree.  Historically, tree search algorithms in games used min-max with alpha-beta pruning.  That is they use a depth-first search to a certain level of depth, hand written heuristics, heavy pruning, and other optimizations—very similar to the tree searches methods in ATPs.  In contrast, Monte Carlo Tree Search (MCTS) explores paths through the tree “at random” (formally it uses UCBT and random rollouts) avoiding the need for hand-written heuristics.  The AlphaZero variant of MCTS eliminates the need to explore to the end of the tree, instead using policy and value heuristics learned by a neural network to guide the search towards nodes in the tree that look “valuable”.  Eliminating the need to explore to the end makes AlphaZero MCTS useful in other tree and graph search problems with sparse terminal states, such as theorem proving.  Indeed, most theorem proving settings can be thought of as tree search problems.  The nodes in the tree are partially constructed proofs and edges are manipulations which lead to the new partial proof.  For example, in tactic-based ITPs, the nodes correspond to a sets of open goals and the edges correspond to tactics which can be applied to the goals.  In other settings, the nodes are are partially built proof trees or tableaus and the edges correspond to inference rules (in say a cut-free sequent calculus).  Many papers in AI and theorem proving are starting to experiment with MCTS.  For example, the TacticToe paper (the most recent version) uses MCTS, but with hand-engineered heuristics.</p>
<p>Another important trend in theorem proving is reinforcement learning (RL), that is learning by exploring the space instead of using labeled data.  This is important since as many have stated there isn’t a lot of human labeled data to use in theorem proving.  Recently, there have been some successful experiments in this area: <a href="https://arxiv.org/abs/1805.07563" target="_blank" title="https://arxiv.org/abs/1805.07563">rlCOP</a> (slides/video at <a href="http://aitp-conference.org/2018/" target="_blank" title="http://aitp-conference.org/2018/">AITP 2018</a>), <a href="https://arxiv.org/abs/1807.08058" target="_blank" title="https://arxiv.org/abs/1807.08058">an RL-trained QBF solver</a>, and <a href="https://arxiv.org/abs/1811.00796v1" target="_blank" title="https://arxiv.org/abs/1811.00796v1">an RL-trained intuitionistic theorem prover</a>.  These results are really encouraging and many of these systems use MCTS and other ideas borrowed from AlphaZero.  Nonetheless, the systems are still quite limited.</p>
<p>There are also applications of AI/ML to SMT solvers, SAT solvers, QBF solvers, resolution theorem provers, connection-style theorem provers, and a variety of different logics.  While there aren’t a lot of researchers directly working on theorem proving, there a number of related research topics including program synthesis, question answering and reasoning in natural language, theorem proving in large relational knowledge databases, and neural-symbolic learning.</p>

<a name="166042086"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042086" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042086">Jason Rute (May 19 2019 at 21:54)</a>:</h4>
<p>I’d also like to share some major trends and challenges I’ve seen in the area of AI for theorem proving.  </p>
<p>Suppose an agent wants to prove a theorem or fill in a gap in a proof.  It could prove it from scratch, but it is much easier to use previously proved facts.  However, if we have a long list of previously proved facts, our combinatorial search space will grow considerably.  This is called the “premise selection problem”.  (For some tactics one also has to specify terms as arguments.  This again is a sort of premise selection problem.)</p>
<p>More powerfully, one would like an agent which creates its own definitions and lemmas.  (This is akin to the difference between a proof system with and without cut, the former allowing arbitrary lemmas.)  All the current benchmarks I am aware of use theorem proving libraries where the intermediate lemmas are already given (or can be chosen from a large list of previously proved results).  I know of no system which has the ability to make its own definitions or lemmas.  (Although in the related field of program synthesis there is work on automatic creation of reusable subroutines.)</p>
<p>Another major trend is feature extraction.  In order to do machine learning on formulas, one has to extract the useful information from the formula.  One time honored tradition is manual “feature engineering”.  While neural networks are really good at automatically discovering features, many of the most successful current systems (e.g. TacticToe) still don’t use neural networks since they are too slow.  Instead they use hand-engineered features in conjunction with faster machine learning methods like boosted trees or nearest-neighbor search.  Nonetheless, I think in the end neural networks will win.  It is already established that neural networks allow one to significantly decrease the search space in many theorem proving tasks, and as we build systems that are more ambitious this exponential speedup in search space will outpace the linear slowdown in feature computation.  While tools like SMT solvers are massively optimized, they only work well in narrow domains.  For research math problems, we need an agent which can learn from the cutting edge without hand engineering.  And we would be willing to wait an hour to solve a problem that would take a week or more for a human to solve.</p>
<p>Even if one uses neural networks to extract features from terms (or formulas, goals, etc.), there are a number of design choices.  While terms (and formulas, etc.) are just strings of symbols, neural networks which are designed for strings of symbols (1D CNNs and LSTMs) don’t seem to work as well.  The reason is that that terms have a deep recursive tree-based structure.  There has been more success with recursive tree neural networks which better capture this recursive structure.  However, formulas aren’t really well represented as trees either because they have bound variables with arbitrary names.  Instead, it seems to be best to use DeBruijn representations where each bound variable is connected to the associated quantifier.  Graph neural networks are amenable to this representation of a term (or formula, etc) as a graph, and seem to do empirically better.  However graph neural networks are still a new concept and more experimentation is likely needed.</p>
<p>Despite all the areas for improvement, I am very hopeful.  I have no doubt that theorem proving is difficult, but incremental progress can have tangible value: Speeding up SMT and SAT solvers.  Learning to quickly prove basic facts in new domains.  Improving the tactics in ITPs to fill in the “obvious parts”.  Improving hardware and software formalization.  Speeding up compilers (even the Lean compiler/type-checker).  This will likely come before automatically proving Math Olympiad problems, auto-formalizing arXiv, having a computer develop new areas of mathematics, or solving really hard research problems.</p>
<p>Another reason to be hopeful is that the tools being built in AI are amazingly general.  For example, two of the important tools used in AlphaStar (DeepMind’s recent Starcraft II AI) are transformers and LSTMs.  Both of these were developed for sequence-to-sequence learning such as machine translation.  Nonetheless, they also turned out to be very applicable to playing real-time strategy games and a host of other problems such as creating computer generated images.  This reusability of tools is a big win.  Researchers don’t need to be working on theorem proving in particular to develop ideas, such as MCTS and graph neural networks, which can be repurposed for theorem proving.  Indeed the problems plaguing AI and theorem proving, such as an infinite search space, are also problems in many other areas of AI.  (Just search for “the bigger picture” in <a href="https://cbmm.mit.edu/video/power-self-learning-systems" target="_blank" title="https://cbmm.mit.edu/video/power-self-learning-systems">this DeepMind talk</a>).  Moreover, I think many of the advanced ideas in AI haven’t even been tried in theorem proving (neither the HOList nor GamePad baselines use graph neural networks for example), which is why I am very excited that the big players (e.g. DeepMind) are joining the field since they have a lot of experience and have shown a willingness to tackle ambitious projects.  Now we just need to convince them that this is worth the payoff!</p>
<p>And finally it seems that, by using reinforcement learning and lots of computer power, we can quickly scale up from ok progress on an AI task to great progress.  Thinking big (maybe too big), if we can teach an AI agent how to make definitions and lemmas, how to use curiosity to explore a new mathematical subject, and how to effectively leverage previous proved knowledge to build new knowledge, then we might—just might—get really impressive results…</p>

<a name="166042146"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042146" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042146">Jason Rute (May 19 2019 at 21:55)</a>:</h4>
<p>(Sorry for the long posts.  As Jeremy can attest to, I haven't learned the skill of short writing.)</p>

<a name="166042194"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042194" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042194">Jason Rute (May 19 2019 at 21:56)</a>:</h4>
<p>(Also, I know many of these resources have been shared many times.  I thought there was value in putting them together in the same place.)</p>

<a name="166042216"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042216" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042216">Kevin Buzzard (May 19 2019 at 21:57)</a>:</h4>
<p>Thanks a lot for these -- I find myself a few times a week on an exercise machine and it's great to have stuff to watch there -- I still haven't figured out how to do Lean whilst pedalling energetically.</p>

<a name="166042393"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042393" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042393">Kevin Buzzard (May 19 2019 at 22:01)</a>:</h4>
<p>Making mathematical definitions is an art -- it's analogous to making new tools. Getting computers to prove theorems using a given set of tools sounds like a much easier problem, and even that seems very difficult to do at the minute.</p>

<a name="166042445"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042445" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042445">Kevin Buzzard (May 19 2019 at 22:02)</a>:</h4>
<p>Brian Conrad is fond of challenging me to get my computer to invent / discover a proof that there are infinitely many primes, and even given the fact that the integers are a unique factorization domain I think a computer would find this hard to do.</p>

<a name="166042891"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166042891" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166042891">Jason Rute (May 19 2019 at 22:15)</a>:</h4>
<p>I agree discovering definitions and "interesting" results will be one of the last things we figure out how to do well.  As far as making new tools, I think to scale automatic theorem proving, we will have to figure out how to build new tactics or make simple lemmas that factor out common proof steps.  This is related to other problems in AI such as hierarchical reinforcement learning where one wants to learn new high level actions made up of low level actions.  Theorem proving is interesting in that the ability to package small steps into one big step is built into the theorem proving framework already, but it is not clear how one would take advantage of it.</p>

<a name="166043229"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166043229" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166043229">Jason Rute (May 19 2019 at 22:24)</a>:</h4>
<p>But speaking of hierarchical RL, the <a href="https://openai.com/blog/openai-five/" target="_blank" title="https://openai.com/blog/openai-five/">OpenAI Five</a> Dota AI is an interesting example.  They thought they would have to solve hierarchical reinforcement learning before they could solve Dota, but it turns out the existing tools (namely LSTMs) already were capable of learning complicated strategies which are executed over a very long time period.  So maybe lemmas and hierarchical RL is not as important as I think.</p>

<a name="166043629"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166043629" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166043629">Moses Schönfinkel (May 19 2019 at 22:36)</a>:</h4>
<p>And yet it didn't even learn the neutral-camp pull into lane! ;) Thanks for the write-up.</p>

<a name="166045274"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166045274" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166045274">Jesse Michael Han (May 19 2019 at 23:21)</a>:</h4>
<p>This is a nice summary. It would be nice to compare the rlCoP-style approach of guiding tableaux/superposition calculus proof search with the <code>tidy_on_steroids</code> approach of TacticToe/HOList, but it doesn't seem like they've been implemented for the same systems yet.</p>

<a name="166056603"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166056603" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166056603">Jason Rute (May 20 2019 at 04:54)</a>:</h4>
<p>The short answer to your question is that the main ideas of the systems are similar.  Connection tableau or tactic-goals, it is basically the same idea.  We progressively modify the proof goal until it is solved.  The devil is in the details.  These details include generating human data by recording proofs and hooking up the theorem prover to a machine learning library.  Even after that, there are a number of challenges with learning tactics since they take arguments and that can greatly expand your search space.</p>
<p><strong>rlCoP</strong></p>
<ul>
<li>Logic: Classical FOL</li>
<li>Search space: <ul>
<li>States: Connection tableau</li>
<li>Actions: Inference rules (cut-free so limited set of valid rules to choose from)</li>
</ul>
</li>
<li>Search algorithm:<ul>
<li>AlphaZero style Monte Carlo tree search (with learned policy/value)</li>
</ul>
</li>
<li>Machine learning method and training algorithm:<ul>
<li>Hand-engineered features fed into XGBoost</li>
<li>AlphaZero style reinforcement learning</li>
<li>It was given first order statements to solve from a list of problems. If it could solve them, then it was rewarded, else punished.  (This is not exactly true, especially for the policy.)</li>
</ul>
</li>
<li>Dataset: Miz40<ul>
<li>Only the statements, not the proofs.</li>
<li>It used RL to learn proofs.</li>
</ul>
</li>
<li>What prevents this from being applied to Lean?<ul>
<li>If used as a first order theorem prover, it would have to be ported from OCaml to Lean. (The most difficult would be the bindings to a machine learning tool to do XGBoost.)</li>
<li>If used as a tactic theorem prover,  the training method is very extensible to tactics and goals.  The main differences are:<ul>
<li>The search space with tactics isn’t necessarily cut-free so it can be quite larger, but it might be possible to fix a smaller subset of tactics (and tactic arguments).</li>
<li>One needs to redo the hand-engineered features (and obviously the RL training).</li>
<li>One needs a good diverse set of training problems of varying difficulty.  (Just theorem statements, not proofs.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>TacticToe</strong></p>
<ul>
<li>System: HOL4 tactics</li>
<li>Search space:<ul>
<li>States: Lists of open goals (plus meta information such as how many times each tactic has been applied)</li>
<li>Actions: Tactics (but this is complicated since tactics have arguments and tactics may fail or do nothing)</li>
</ul>
</li>
<li>Search algorithm:<ul>
<li>AlphaZero style Monte Carlo tree search (with policy/value)</li>
<li>The policy and value cover special tactics like applying an ATP or applying tactics which take arguments.</li>
</ul>
</li>
<li>Machine learning method and training algorithm:<ul>
<li>Curated database of tactics</li>
<li>Hand engineered “predictor” features</li>
<li>“Learning” means creating and improving this database</li>
<li>There is a lot I don’t understand yet about the learning step</li>
<li>Policy / value are based on how similar the current tactic / goal is to what is in the database based on the hand-engineered predictor features.</li>
</ul>
</li>
<li>Dataset: <ul>
<li>“Human proofs” (I assume the HOL4 library, with proper proof recording)</li>
</ul>
</li>
<li>What prevents this from being applied to Lean?<ul>
<li>It is probably mostly just engineering, but there is a lot of engineering involved.</li>
<li>I don’t know if the differences in logic really matter.</li>
</ul>
</li>
</ul>
<p><em>GamePad</em></p>
<ul>
<li>System: Coq tactics</li>
<li>Search space:<ul>
<li>States: Lists of open goals</li>
<li>Actions: Tactics (they didn’t deal with the complicated case of the have tactic or the rewrite tactic which take arguments)</li>
</ul>
</li>
<li>Search algorithm:<ul>
<li>The GamePad baselines don’t have a proof search, but the whole point of GamePad it is a framework for one two build machine learning tools for Coq such as Monte Carlo tree search.</li>
<li>While they don’t do full theorem proving, they do predict tactics used in human proofs and proof evaluation (number of steps left in the proof) which could be used for a tree search algorithm.</li>
</ul>
</li>
<li>Machine learning method and training algorithm:<ul>
<li>Recursive tree neural networks learn embeddings (feature vectors encoding information) for terms, formulas, goals, etc.</li>
<li>Tried a number of machine learning methods (SVM, GRU, LSTM) for the prediction and evaluation tasks. </li>
<li>Trained with supervised learning</li>
</ul>
</li>
<li>Dataset: <ul>
<li>Coq Library, Feit Thompson</li>
</ul>
</li>
<li>What prevents this from being applied to Lean?<ul>
<li>This is closest to Lean in logic.  Again, it’s a lot of engineering.  </li>
<li>Also, there is no prover built yet.  This is more of a learning framework than a final product.  (The goal is for others to work in this framework and beat their baselines.)</li>
</ul>
</li>
</ul>
<p><em>HOList</em></p>
<ul>
<li>System: HOL Light tactics</li>
<li>Search space:<ul>
<li>States: Goal (set of subgoals if I am not mistaken)</li>
<li>Actions: Tactics (both tactic and premise argument)</li>
</ul>
</li>
<li>Search algorithm:<ul>
<li>Breath first search</li>
<li>Limit on the number of tactic actions from each goal</li>
<li>Tactics are ranked by neural network policy</li>
<li>Again, this baseline algorithm is designed to be improved by other users.</li>
</ul>
</li>
<li>Machine learning method and training algorithm:<ul>
<li>Neural network embeddings for goals and premises.  (I think they tried both convolutional NNs and WaveNet, but I don’t know for sure.)</li>
<li>Neural networks which rank tactics and tactic + premise pairs.  (Again, I’m not clear on the details.)</li>
<li>Trained with supervised learning first on human proof and then reinforcement learning.  (I’m not clear on the RL training details.)</li>
</ul>
</li>
<li>Datasets: <ul>
<li>HOL Light core, complex (as in complex arithmetic), and Flyspeck</li>
</ul>
</li>
<li>What prevents this from being applied to Lean?<ul>
<li>Again, it’s a lot of engineering.  </li>
<li>It could be possible to hook up their system to Lean in theory.  Of course the exact logic and tactics are different.</li>
</ul>
</li>
</ul>

<a name="166075620"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166075620" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166075620">Joseph Corneli (May 20 2019 at 11:06)</a>:</h4>
<p>Here are a couple references I've come across that look relevant to premise selection (in the context of proof synthesis).</p>
<p>A type-theoretic approach that could be germane to our style of working here:</p>
<p>Jan Bessai et al. Combinatory Logic Synthesizer. In Tiziana Margaria and Bernhard Steffen, editors, Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change, pages 26–40, Berlin, Heidelberg. Springer Berlin Hei- delberg, 2014.</p>
<p>Applications of contemporary ML tools to code:</p>
<p>Tal Ben-Nun et al. Neural Code Comprehension: A Learnable Representation of Code Semantics. In S. Bengio et al., editors, Advances in Neural Information Processing Systems 31, pages 3589–3601. Curran Associates, Inc., 2018.</p>

<a name="166863717"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/166863717" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#166863717">Jason Rute (May 29 2019 at 22:36)</a>:</h4>
<p>I’ve recently come across these three new papers on machine learning theorem proving in interactive theorem provers:</p>
<ul>
<li><a href="https://arxiv.org/abs/1905.09381" target="_blank" title="https://arxiv.org/abs/1905.09381">Learning to Prove Theorems via Interacting with Proof Assistants</a>  A theorem prover (ASTactic) and gym (CoqGym) for Coq.  (Separate project from GamePad.)  Trained on a very large collection of Coq projects.</li>
<li>These next two are new additions to the <a href="https://sites.google.com/view/holist/home" target="_blank" title="https://sites.google.com/view/holist/home">HOList</a> project for HOL Light which now has a nice summary of all their results at the top of the website.<ul>
<li><a href="https://arxiv.org/abs/1905.10006" target="_blank" title="https://arxiv.org/abs/1905.10006">Graph Representations for Higher-Order Logic and Theorem Proving</a>  An improved automated theorem prover using graph neural networks.  It is trained on human proofs.</li>
<li><a href="https://arxiv.org/abs/1905.10501" target="_blank" title="https://arxiv.org/abs/1905.10501">Learning to Reason in Large Theories without Imitation</a>  An improved reinforcement learning theorem prover trained without human proofs (but trained with human written theorem statements and uses standard HOL Light human engineered tactics).</li>
</ul>
</li>
</ul>

<a name="167625175"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/167625175" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#167625175">Bryan Gin-ge Chen (Jun 08 2019 at 00:32)</a>:</h4>
<p>I haven't watched it yet, but here's an interview with Christian Szegedy of the Google Brain team. Presumably he talks about some of the papers mentioned above. <a href="https://www.youtube.com/watch?v=p_UXra-_ORQ" target="_blank" title="https://www.youtube.com/watch?v=p_UXra-_ORQ">https://www.youtube.com/watch?v=p_UXra-_ORQ</a></p>
<div class="youtube-video message_inline_image"><a data-id="p_UXra-_ORQ" href="https://www.youtube.com/watch?v=p_UXra-_ORQ" target="_blank" title="https://www.youtube.com/watch?v=p_UXra-_ORQ"><img src="https://i.ytimg.com/vi/p_UXra-_ORQ/default.jpg"></a></div>

<a name="167696394"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/AI%20and%20theorem%20proving/near/167696394" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/41431AIandtheoremproving.html#167696394">Jason Rute (Jun 09 2019 at 11:41)</a>:</h4>
<p><span class="user-mention" data-user-id="123965">@Bryan Gin-ge Chen</span> Thanks for sharing!  It was really interesting!</p>
<p>For those who prefer to read, here is <a href="https://scale.ai/interviews/christian-szegedy/transcript" target="_blank" title="https://scale.ai/interviews/christian-szegedy/transcript">a (computer generated) transcript of the video</a>.  It's a non-technical, high-level vision talk and is very accessible to the Lean community.  </p>
<ul>
<li>The first part of the video is about how Christian got into deep learning about about his earlier work (which is quite influential in the field, batch-normalization, inception architecture, and adversarial examples).  </li>
<li>At 14:26, they start talking about AI and formalization and Szegedy explains his motivation which is high level automated code generation. </li>
<li>At 19:33 the interviewer challenges Szegedy about why he thinks AI mathematics is at all tractable.  Szegedy has some interesting thoughts and moreover has a much bigger vision than I realized.  He thinks the only way to make this tractable is to simultaneously work on automated formalization of books and papers while also working on automated theorem proving.  Both will strengthen the other (and have huge consequences in natural language understanding if successful).  </li>
<li>At 26:43 they talk about the HOList project (but not by name), and what papers are coming next in a few weeks from his lab.  Rather than using a fix search algorithm like beam search, the AI agent will handle the searching as well.  (I've been thinking a lot about this same thing recently.  MCTS works really well in games since even small differences in game states can be exploited by an opponent, but in theorem proving, there are many paths to the goal and exploring one branch will tell you a lot about similar branches in a way that beam search or MCTS don't handle.)</li>
</ul>


{% endraw %}
