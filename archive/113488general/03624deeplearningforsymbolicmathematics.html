---
layout: archive
title: Lean Prover Zulip Chat Archive
permalink: archive/113488general/03624deeplearningforsymbolicmathematics.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/113488general/index.html">general</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html">deep learning for symbolic mathematics</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">

{% raw %}
<a name="185657086"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185657086" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185657086">Tim Daly (Jan 15 2020 at 00:39)</a>:</h4>
<p>Lample Guillaume and Charton, Francois "Deep Learning for Symbolic Mathematics"  (<a href="https://arxiv.org/pdf/1912.01412.pdf" target="_blank" title="https://arxiv.org/pdf/1912.01412.pdf">https://arxiv.org/pdf/1912.01412.pdf</a></p>

<a name="185657756"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185657756" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185657756">Tim Daly (Jan 15 2020 at 00:52)</a>:</h4>
<p>I didn't think this was possible and I was wrong. Now the question is, can similar techniques apply to Lean? They did have the advantage that they could generate large datasets because they could generate a random equation, differentiate it, and then know that the integral exists. I'm not sure how to generate "random" proofs.</p>

<a name="185658084"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185658084" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185658084">Tim Daly (Jan 15 2020 at 00:59)</a>:</h4>
<p>Their technique of Integerate(Diff(f(x),x)) also generates a lot of random test cases (fuzzing) useful for debugging. So if there was a similar hack for Lean then one could build a fuzz tester for Lean.</p>
<p>The only thing I can think of is there is the idea of "generate all proofs of length N" (usually used for the idea of exhaustive search). I don't know how to generate proofs of length N though.</p>

<a name="185658500"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185658500" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185658500">Simon Cruanes (Jan 15 2020 at 01:07)</a>:</h4>
<p>If you consider the set of rules of some sequent calculus (like HOL's kernel), you can use them bottom-to-top as generators as if they were prolog rules, I think.<br>
From</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">Γ</mi><mo separator="true">,</mo><mi>A</mi><mo>⊢</mo><mi>B</mi></mrow><mrow><mi mathvariant="normal">Γ</mi><mo>⊢</mo><mi>A</mi><mo>⇒</mo><mi>B</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\cfrac{\Gamma, A \vdash B}{\Gamma \vdash A \Rightarrow B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.276em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5899999999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">Γ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⊢</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⇒</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">Γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⊢</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p>
<p>you can start with a formula <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>φ</mi></mrow><annotation encoding="application/x-tex">\varphi</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">φ</span></span></span></span> and unify it with</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⇒</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A \Rightarrow B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⇒</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span></span></p>
<p>and recursively unify <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span> with the conclusions of other rules?<br>
I know it's doable at least for generating terms with ML-style polymorphic types, I've used that in the past for quickcheck stuff.</p>
<p>For Lean you'd run the refiner "in reverse", in a similar way.</p>

<a name="185658846"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185658846" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185658846">Tim Daly (Jan 15 2020 at 01:14)</a>:</h4>
<p>In the paper they create a tree structure by moving the "operator" to the prefix position (e.g. 2 + 3 becomes (+ 2 3). So A \Rightarrow B becomes (\Rightarrow A B) in their generator. It should be possible to create a generator using that tree-method.</p>

<a name="185659173"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185659173" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185659173">Tim Daly (Jan 15 2020 at 01:20)</a>:</h4>
<p>The next hack would be to have a set of tactics that were exactly the rules in Lean. Then, when you generate a theorem (in the forward direction by creating a proof tree) you can look at the theorem and apply the tactics to reconstruct the proof.  Since you know the theorem is true, and you know the tactics cover the set of rules used to generate the proof, Lean should be able to (automatically?) prove the theorem.<br>
Maybe. Possibly.</p>

<a name="185659289"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185659289" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185659289">Tim Daly (Jan 15 2020 at 01:23)</a>:</h4>
<p>Once you have a large enough training set then perhaps Lample and Charton can run their ML program on it and "learn" to prove.</p>

<a name="185723762"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185723762" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185723762">Bryan Gin-ge Chen (Jan 15 2020 at 17:50)</a>:</h4>
<p>There's an interesting discussion on the paper of Lample and Charton in this sage-devel thread: <a href="https://groups.google.com/forum/#!topic/sage-devel/eXMbHpG3_oY" target="_blank" title="https://groups.google.com/forum/#!topic/sage-devel/eXMbHpG3_oY">https://groups.google.com/forum/#!topic/sage-devel/eXMbHpG3_oY</a></p>
<p>It links to a "review" by Ernest Davis on the arxiv: <a href="https://arxiv.org/abs/1912.05752" target="_blank" title="https://arxiv.org/abs/1912.05752">https://arxiv.org/abs/1912.05752</a> and this blog post by Brent Baccala (featuring Axiom!): <a href="https://www.freesoft.org/blogs/soapbox/the-facebook-integral/" target="_blank" title="https://www.freesoft.org/blogs/soapbox/the-facebook-integral/">https://www.freesoft.org/blogs/soapbox/the-facebook-integral/</a></p>

<a name="185915040"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185915040" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185915040">Jason Rute (Jan 17 2020 at 13:23)</a>:</h4>
<p>As for the integration paper, I think it is fascinating, but as others have pointed out, one needs to take the results with a some skepticism.  However, competing with highly engineered and optimized CAS systems will be a hard challenge.  I think the results are encouraging and they could have applications in speeding up and extending CAS systems.</p>
<p>I think a more interesting application to this technique would be to find a mathematical domain (less studied than integration) where one direction of the problem is easy and the reverse direction is hard (but still someone with a lot of experience could often figure out the reverse answer from intuition, memory, and light reasoning), and verifying that the answer in the reverse direction is correct is fairly easy in practice.  It is possible that techniques like this could have a huge benefit.  However, I'm struggling to find good examples.</p>
<p>As for using this on theorem proving, that would fit my description above (maybe), but it also seems like a particular challenging problem to expect an end-to-end sequence-to-sequence model to just work on anything but trivial cases.  (However, it might still be worth giving it a try!)  On a more techincal level, tactic proofs are not unique.  For example, <code>by simp</code> is the proof of a number of different goals.  Even <code>by refl</code> is not unique (but is quite common).  I haven't thought about it too much, but maybe some subset of the term language would suffice.  Of course, one could also try this out in other proof languages besides Lean where proofs uniquely specify their theorems (or where synthetic proofs can at least be easily generated from rules/axioms).  (Metamath, cut-free sequent calculi, hilbert-style deduction, tableau  calculus)</p>
<p>Setting aside this exact method of Lample and Charton, I could also imagine generating synthetic proofs could be an important part of a synthetic method to train machine learning provers, especially combined with reinforcement learning.  Already the <a href="https://sites.google.com/view/holist/home" target="_blank" title="https://sites.google.com/view/holist/home">DeepHOL/HOList work</a> and this <a href="https://openreview.net/forum?id=BJxiqxSYPB" target="_blank" title="https://openreview.net/forum?id=BJxiqxSYPB">Metamath AI</a> augment their training data with synthetically generated theorems/proofs.  I could imagine an advanced system which has one agent whose goal is to generate realistic looking synthetic theorems/proofs which are then fed to another agent for training on them.</p>

<a name="185915821"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185915821" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185915821">Johan Commelin (Jan 17 2020 at 13:33)</a>:</h4>
<blockquote>
<p>I think a more interesting application to this technique would be to find a mathematical domain (less studied than integration) where one direction of the problem is easy and the reverse direction is hard (but still someone with a lot of experience could often figure out the reverse answer from intuition, memory, and light reasoning), and verifying that the answer in the reverse direction is correct is fairly easy in practice.  It is possible that techniques like this could have a huge benefit.  However, I'm struggling to find good examples.</p>
</blockquote>
<p>Factoring numbers or polynomials?</p>

<a name="186351249"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/186351249" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#186351249">Tim Daly (Jan 23 2020 at 02:00)</a>:</h4>
<p>Wow. I go away for a week and somebody finds Axiom in the wild. Sweet. I have been reviewing that paper and using their ideas for fuzz testing.</p>

<a name="186351554"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/186351554" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#186351554">Tim Daly (Jan 23 2020 at 02:08)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> You might find Axiom's Computer Algebra Test Suite (<a href="http://axiom-developer.org/axiom-website/CATS/index.html" target="_blank" title="http://axiom-developer.org/axiom-website/CATS/index.html">http://axiom-developer.org/axiom-website/CATS/index.html</a>) which I've been constructing from publications.</p>

<a name="186354170"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/186354170" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#186354170">Tim Daly (Jan 23 2020 at 03:17)</a>:</h4>
<p>Wow. I watched those videos showing Facebook vs Axiom vs Sage. What a masterful presentation!</p>
<p>Axiom's Risch algorithm is (I believe) the most complete implementation although there is still much to do. The algorithm was the work of Barry Trager and Manuel Bronstein.</p>
<p>The take-away for Lean is that there are 2 branches of computational mathematics, proof systems and computer algebra. The computer algebra branch is fairly deep in several areas. Axiom uses group theory as scaffold to build algorithms like Risch. Lean also uses group theory as an underlying structure. I hope it will be possible to unify these two branches of computational mathematics so one can compute with trusted algorithms.</p>

<a name="186355461"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/186355461" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#186355461">Tim Daly (Jan 23 2020 at 03:55)</a>:</h4>
<p>The second video on the Risch Algorithm is deeply related to Algebraic Geometry.</p>


{% endraw %}

{% include archive_update.html %}